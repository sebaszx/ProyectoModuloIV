# -*- coding: utf-8 -*-
"""Proyecto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17crHwCOpinFWOm9is-LgMHIxHWgslK9A
"""

#import findspark
#findspark.init()

#import pyspark

from pyspark.sql import SparkSession
from pyspark.sql.types import StructField,StringType,IntegerType,StructType,DateType,FloatType
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.ml.stat import Correlation
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

spark = SparkSession \
    .builder \
    .appName("Proyecto") \
    .config("spark.driver.extraClassPath", "postgresql-42.2.14.jar") \
    .config("spark.executor.extraClassPath", "postgresql-42.2.14.jar") \
    .getOrCreate()

"""Se leen los datos de los accidentes"""

accidentsUSAFull = spark \
    .read \
    .format("csv") \
    .option("path", "Datos/US_Accidents_Dec21_updated_solo_2021.csv") \
    .option("header", True) \
    .schema(StructType([
                StructField("ID", StringType()),
                StructField("Severity", IntegerType()),
                StructField("Start_Time", StringType()),
                StructField("End_Time", StringType()),
                StructField("Start_Lat", FloatType()),
                StructField("Start_Lng", FloatType()),
                StructField("End_Lat", FloatType()),
                StructField("End_Lng", FloatType()),
                StructField("Distance(mi)", FloatType()),
                StructField("Description", StringType()),
                StructField("Number", IntegerType()),
                StructField("Street", StringType()),
                StructField("Side", StringType()),
                StructField("City", StringType()),
                StructField("County", StringType()),
                StructField("State", StringType()),
                StructField("Zipcode", StringType()),
                StructField("Country", StringType()),
                StructField("Timezone", StringType()),
                StructField("Airport_Code", StringType()),
                StructField("Weather_Timestamp", StringType()),
                StructField("Temperature(F)", FloatType()),
                StructField("Wind_Chill(F)", FloatType()),
                StructField("Humidity(%)", FloatType()),
                StructField("Pressure(in)", FloatType()),
                StructField("Visibility(mi)", FloatType()),
                StructField("Wind_Direction", StringType()),
                StructField("Wind_Speed(mph)", FloatType()),
                StructField("Precipitation(in)", FloatType()),
                StructField("Weather_Condition", StringType())
                ])) \
    .load()
#accidentsUSAFull.select(to_timestamp(accidentsUSAFull.Start_Time, 'MM-dd-YYYY HH:mm:ss').alias('dt')).show(1)
#accidentsUSAFull.printSchema()
accidentsUSAFull= accidentsUSAFull.withColumn('col_with_date_format',F.to_date(accidentsUSAFull.Start_Time))
#below is the result
accidentsUSAFull.select('Start_Time','col_with_date_format').show(10,False)
#accidentsUSAFull.select(to_timestamp(accidentsUSAFull.Start_Time, 'MM-dd-yyyy HH:mm').alias('dt')).collect()

"""Filtramos un poco el dataframe para solo obtener las columnas de nuestro interes"""

accidentsUSA=accidentsUSAFull.select(accidentsUSAFull["Severity"],
accidentsUSAFull["Start_Time"],accidentsUSAFull["End_Time"],accidentsUSAFull["City"],
accidentsUSAFull["County"],accidentsUSAFull["State"],accidentsUSAFull["Temperature(F)"],
accidentsUSAFull["Wind_Chill(F)"],
accidentsUSAFull["Humidity(%)"],accidentsUSAFull["Pressure(in)"],accidentsUSAFull["Visibility(mi)"],
accidentsUSAFull["Zipcode"])

"""Hacemos un describe de los datos"""

accidentsUSA.describe().show()

"""Contamos los valores nullos"""

from pyspark.sql.functions import col,isnan, when, count
accidentsUSA.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in accidentsUSA.columns]
   ).show()

"""Se limpia el dataset"""

accidentsUSA=accidentsUSA.na.drop()

from pyspark.sql.functions import col,isnan, when, count
accidentsUSA.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in accidentsUSA.columns]
   ).show()

"""Se lee el archivo del clima"""

WeatherFull = spark \
    .read \
    .format("csv") \
    .option("path", "Datos/WeatherEvents_Jan2016-Dec2021_solo_2021.csv") \
    .option("header", True) \
    .schema(StructType([
                StructField("EventId", StringType()),
                StructField("Type", StringType()),
                StructField("Severity", StringType()),
                StructField("StartTime(UTC)", StringType()),
                StructField("EndTime(UTC)", StringType()),
                StructField("Precipitation(in)", FloatType()),
                StructField("TimeZone", StringType()),
                StructField("AirportCode", StringType()),
                StructField("LocationLat", FloatType()),
                 StructField("LocationLng", FloatType()),
                StructField("City", StringType()),
                StructField("County", StringType()),
                StructField("State", StringType()),
                StructField("ZipCode", StringType()),
               
                ])) \
    .load()
WeatherFull.show()

"""Simplificamos el dataset con lo que necesitamos"""

weatherUSA=WeatherFull.select(WeatherFull["Type"],
WeatherFull["Severity"],WeatherFull["StartTime(UTC)"],WeatherFull["EndTime(UTC)"],
WeatherFull["Precipitation(in)"],WeatherFull["State"],WeatherFull["County"],
WeatherFull["City"],
WeatherFull["Zipcode"])

weatherUSA.describe().show()

weatherUSA.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in weatherUSA.columns]
   ).show()

"""Se limpia el dataset"""

weatherUSA=weatherUSA.na.drop()

weatherUSA.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in weatherUSA.columns]
   ).show()

   
"""Intenamos balancear el target"""
weatherUSA.groupBy("Type").count().show()

weatherUSA = weatherUSA.withColumn("Type", when(weatherUSA.Type == "Cold","Others") \
      .when(weatherUSA.Type == "Storm","Others") \
      .when(weatherUSA.Type == "Hail","Others") \
        .when(weatherUSA.Type == "Precipitation","Others") \
      .otherwise(weatherUSA.Type))
weatherUSA.show()

weatherUSA.groupBy("Type").count().show()

n = 8984
weatherUSA = weatherUSA.withColumn('rand_col', F.rand())
weatherUSA = weatherUSA.withColumn("row_num",F.row_number().over(Window.partitionBy("Type")\
                .orderBy("rand_col"))).filter(col("row_num")<=n)\
                .drop("rand_col", "row_num")

weatherUSA.groupBy("Type").count().show()

"""EDA"""
"""


x=accidentsUSA.toPandas()["Severity"].values.tolist()

pd.Series(x).value_counts(sort=False).plot(kind='bar')

y=accidentsUSA.toPandas()["Visibility(mi)"].values.tolist()
x = np.arange(0, len(y))
print(len(y))
plt.title("Visibilidad en millas")

plt.plot(x, y, color ="green")
plt.show()

x=weatherUSA.toPandas()["Type"].values.tolist()

pd.Series(x).value_counts(sort=False).plot(kind='bar')

x=weatherUSA.toPandas()["Severity"].values.tolist()

pd.Series(x).value_counts(sort=False).plot(kind='bar')

"""


"""Union de dataframes

"""
FullJoinDF=weatherUSA.join(accidentsUSA, (weatherUSA["City"] == accidentsUSA["City"]) &
   ( weatherUSA["County"] == accidentsUSA["County"])  &
   ( weatherUSA["State"] == accidentsUSA["State"])  &
   ( weatherUSA["ZipCode"] == accidentsUSA["ZipCode"]) 
   &
   ( weatherUSA["StartTime(UTC)"] == accidentsUSA["Start_Time"]),"inner")
rows=FullJoinDF.count()
print(rows)

FullJoinDF.printSchema()
